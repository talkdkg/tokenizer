<article>
	<h1>Tokenizer: The Web Crawler</h1>
	<p>A Web crawler is an Internet bot that systematically browses the
		World
		Wide Web, typically for the purpose of Web indexing.
		A Web crawler
		may
		also be called a Web spider, a web scraper, a web indexer.
		Web
		search engines and some
		other sites use Web crawling or spidering
		software to update their web
		content or indexes of others sites' web
		content. Web crawlers can copy
		all the pages they visit for later
		processing by a search engine that
		indexes the downloaded pages so that
		users can search them much more
		quickly.
		Crawlers can validate
		hyperlinks and HTML code. They can also be
		used for web scraping.
	</p>


	<h2>Web Crawling</h2>
	<p>
		Classic web crawler starts with a list of URLs to visit, called the
		seeds. As
		the crawler visits these URLs, it identifies all the
		hyperlinks in the
		page and adds them to the list of URLs to visit,
		called the crawl
		frontier. URLs from the frontier are recursively
		visited according to
		a set of policies.
	</p>
	<p>
		The large volume implies that the crawler can only download a
		limited
		number of the Web pages within a given time, so it needs to
		prioritize
		its downloads. The high rate of change implies that the
		pages might
		have already been updated or even deleted.
	</p>
	<p>
		The number of possible crawlable URLs being generated by server-side
		software has also made it difficult for web crawlers to avoid
		retrieving duplicate content. Endless combinations of HTTP GET
		(URL-based) parameters exist, of which only a small selection will
		actually return unique content.
		Given that the bandwidth for conducting
		crawls
		is neither infinite nor free, it is becoming essential to crawl
		the
		Web in not only a scalable, but efficient way. A crawler
		must
		carefully choose at each step which pages to visit next.
		The behavior
		of a Web crawler is the outcome of a combination of
		policies:
		<ul>
			<li>
				a selection policy that states which pages to download
			</li>
			<li>a re-visit policy that states when to check for changes to the
				pages
			</li>
			<li>a politeness policy that states how to avoid overloading Web
				sites
			</li>
			<li>a parallelization policy that states how to coordinate
				distributed web crawlers
			</li>


		</ul>

	</p>

	<h2>Tokenizer Web Crawler Specifics</h2>

	Based upon our Elastic Executor framework, our crawler consists from
	few configurable tasks, each of them can be clustered singleton or
	multiton (such as N processes in a cluster of K nodes with automatic
	failover), each of them can be started, stopped, and reconfigured in a
	cluster of 1000+ nodes in real time. Compare with traditional
	technology such as MapReduce, Hadoop, BIXO, and Nutch!

	We have
	implemented following tasks:
	<ul>
		<li>
			Vertical Crawl - collect everything from a specified Internet host
		</li>
		<li>
			Sitemaps Based Crawl - collect pages listed in a sitemaps from a
			specified
			Internet host
		</li>
		<li>
			Feeds Subscriber - subscribe to RSS, feeds from Datasift/Twitter,
			Facebook, Moreover, and more
		</li>

	</ul>





</article>
